pyspark.sql.functions.split

Splits str around matches of the given pattern.

Parameters
str : Column or str
        a string expression to split

pattern : str
        a string representing a regular expression. The regex string should be a Java regular expression.

limit : int, optional
        an integer which controls the number of times pattern is applied.

        limit > 0: The resulting array’s length will not be more than limit, 
                   and the resulting array’s last entry will contain all input beyond the last matched pattern.

        limit <= 0: pattern will be applied as many times as possible, and the resulting
                    array can be of any size.

Returns : Column
        array of separated strings.

Examples

df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])

df.select(split(df.s, '[ABC]', 2).alias('s')).collect()
[Row(s=['one', 'twoBthreeC'])]

df.select(split(df.s, '[ABC]', -1).alias('s')).collect()
[Row(s=['one', 'two', 'three', ''])]

---------------------------------------------------------------------------------------------------------------------------------------------------

pyspark.sql.functions.split_part

Splits str by delimiter and return requested part of the split (1-based). If any input is null, returns null. if partNum is out of range of split parts, returns empty string. If partNum is 0, throws an error. If partNum is negative, the parts are counted backward from the end of the string. If the delimiter is an empty string, the str is not split.

Parameters
src : Column or str
        A column of string to be splited.

delimiter : Column or str
        A column of string, the delimiter used for split.

partNum : Column or str
        A column of string, requested part of the split (1-based).

Examples

df = spark.createDataFrame([("11.12.13", ".", 3,)], ["a", "b", "c"])

df.select(split_part(df.a, df.b, df.c).alias('r')).collect()

[Row(r='13')]
