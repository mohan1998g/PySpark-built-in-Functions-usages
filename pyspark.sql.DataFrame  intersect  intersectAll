pyspark.sql.DataFrame.intersect

Return a new DataFrame containing rows only in both this DataFrame and another DataFrame. Note that any duplicates are removed. To preserve duplicates use intersectAll().

Parameters
other : DataFrame
        Another DataFrame that needs to be combined.

Returns : DataFrame
        Combined DataFrame.

Notes

This is equivalent to INTERSECT in SQL.

Examples

df1 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3), ("c", 4)], ["C1", "C2"])
df2 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3)], ["C1", "C2"])
df1.intersect(df2).sort(df1.C1.desc()).show()
+---+---+
| C1| C2|
+---+---+
|  b|  3|
|  a|  1|
+---+---+

---------------------------------------------------------------------------------------------

pyspark.sql.DataFrame.intersectAll

Return a new DataFrame containing rows in both this DataFrame and another DataFrame while preserving duplicates.

This is equivalent to INTERSECT ALL in SQL. As standard in SQL, this function resolves columns by position (not by name).

Parameters
other : DataFrame
        Another DataFrame that needs to be combined.

Returns : DataFrame
        Combined DataFrame.

Examples

df1 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3), ("c", 4)], ["C1", "C2"])
df2 = spark.createDataFrame([("a", 1), ("a", 1), ("b", 3)], ["C1", "C2"])
df1.intersectAll(df2).sort("C1", "C2").show()
+---+---+
| C1| C2|
+---+---+
|  a|  1|
|  a|  1|
|  b|  3|
+---+---+
